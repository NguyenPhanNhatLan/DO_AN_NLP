{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Bật cờ để vô hiệu hóa việc tải Metal Plugin\n",
    "os.environ['TF_ENABLE_METAL_PLUGINS'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Dầu, tẩy, trang, Cocoon, hoa, hồng, cho, da, ...</td>\n",
       "      <td>[B-PRODUCT, I-PRODUCT, I-PRODUCT, B-BRAND, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[Tinh, chất, Some, By, Mi, AHA, BHA, giảm, mụn]</td>\n",
       "      <td>[B-PRODUCT, I-PRODUCT, B-BRAND, I-BRAND, I-BRA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[Serum, Balance, Niacinamide, 10%, cho, da, dầ...</td>\n",
       "      <td>[B-PRODUCT, B-BRAND, B-INGREDIENT, I-INGREDIEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[Kem, dưỡng, ẩm, CeraVe, chứa, Ceramide, và, H...</td>\n",
       "      <td>[B-PRODUCT, I-PRODUCT, I-PRODUCT, B-BRAND, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[Toner, Hada, Labo, cấp, ẩm, cho, da, khô, nhạ...</td>\n",
       "      <td>[B-PRODUCT, B-BRAND, I-BRAND, B-BENEFIT, I-BEN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             tokens  \\\n",
       "0   1  [Dầu, tẩy, trang, Cocoon, hoa, hồng, cho, da, ...   \n",
       "1   2    [Tinh, chất, Some, By, Mi, AHA, BHA, giảm, mụn]   \n",
       "2   3  [Serum, Balance, Niacinamide, 10%, cho, da, dầ...   \n",
       "3   4  [Kem, dưỡng, ẩm, CeraVe, chứa, Ceramide, và, H...   \n",
       "4   5  [Toner, Hada, Labo, cấp, ẩm, cho, da, khô, nhạ...   \n",
       "\n",
       "                                                tags  \n",
       "0  [B-PRODUCT, I-PRODUCT, I-PRODUCT, B-BRAND, I-P...  \n",
       "1  [B-PRODUCT, I-PRODUCT, B-BRAND, I-BRAND, I-BRA...  \n",
       "2  [B-PRODUCT, B-BRAND, B-INGREDIENT, I-INGREDIEN...  \n",
       "3  [B-PRODUCT, I-PRODUCT, I-PRODUCT, B-BRAND, O, ...  \n",
       "4  [B-PRODUCT, B-BRAND, I-BRAND, B-BENEFIT, I-BEN...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('ner.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df['tokens'])\n",
    "tags_lists = list(df['tags'])\n",
    "unique_tags = sorted(df['tags'].explode().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: i for i, label in enumerate(unique_tags)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags_lists, tokenizer, label2id, max_len=64):\n",
    "        self.sentences = sentences\n",
    "        self.tags_lists = tags_lists\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = 64\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        words = self.sentences[index]\n",
    "        tags = self.tags_lists[index]\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        encoding.pop(\"offset_mapping\")\n",
    "        \n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                if word_idx != previous_word_idx:\n",
    "                    label_ids.append(self.label2id[tags[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "        encoding[\"labels\"] = label_ids\n",
    "\n",
    "        # convert sang tensor\n",
    "        return {k: torch.tensor(v) for k, v in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    sentences, tags_lists, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(X_train, y_train, tokenizer, label2id)\n",
    "val_dataset   = NERDataset(X_val,   y_val,   tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "\n",
    "    for pred_seq, lab_seq in zip(predictions, labels):\n",
    "        cur_true = []\n",
    "        cur_pred = []\n",
    "        for p_i, l_i in zip(pred_seq, lab_seq):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            cur_true.append(id2label[l_i])\n",
    "            cur_pred.append(id2label[p_i])\n",
    "        if len(cur_true) > 0:\n",
    "            true_tags.append(cur_true)\n",
    "            pred_tags.append(cur_pred)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_tags, pred_tags),\n",
    "        \"recall\":    recall_score(true_tags, pred_tags),\n",
    "        \"f1\":        f1_score(true_tags, pred_tags),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_product_demo\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./ner_logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"all\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kr/kwr_0qbj425dp77_59mypb0w0000gn/T/ipykernel_92200/1177159695.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 04:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.822700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/tf310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=0.9355677392747667, metrics={'train_runtime': 278.373, 'train_samples_per_second': 1.293, 'train_steps_per_second': 0.323, 'total_flos': 11759523425280.0, 'train_loss': 0.9355677392747667, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def ner_predict(text):\n",
    "    # tạm thời split đơn giản theo khoảng trắng (sau này bạn có thể dùng tokenizer tiếng Việt tốt hơn)\n",
    "    words = text.split()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    word_ids = encoding[\"input_ids\"].new_tensor(tokenizer(text.split(), is_split_into_words=True).word_ids())\n",
    "    # Cách khác: dùng encoding.word_ids(batch_index=0) nếu dùng tokenizer fast\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "    entities = []\n",
    "    current_tokens = []\n",
    "    current_label = None\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "\n",
    "        # chỉ lấy subword đầu tiên\n",
    "        if idx > 0 and word_idx == word_ids[idx-1]:\n",
    "            continue\n",
    "\n",
    "        label_id = predictions[idx]\n",
    "        label_name = id2label[label_id]\n",
    "        word = words[word_idx]\n",
    "\n",
    "        if label_name.startswith(\"B-\"):\n",
    "            # kết thúc entity cũ\n",
    "            if current_tokens:\n",
    "                entities.append({\n",
    "                    \"text\": \" \".join(current_tokens),\n",
    "                    \"label\": current_label\n",
    "                })\n",
    "            current_tokens = [word]\n",
    "            current_label = label_name[2:]  # bỏ \"B-\"\n",
    "        elif label_name.startswith(\"I-\") and current_label == label_name[2:]:\n",
    "            current_tokens.append(word)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                entities.append({\n",
    "                    \"text\": \" \".join(current_tokens),\n",
    "                    \"label\": current_label\n",
    "                })\n",
    "                current_tokens = []\n",
    "                current_label = None\n",
    "\n",
    "    # entity cuối\n",
    "    if current_tokens:\n",
    "        entities.append({\n",
    "            \"text\": \" \".join(current_tokens),\n",
    "            \"label\": current_label\n",
    "        })\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDầu tẩy trang Cocoon hoa hồng cho da dầu mụn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ents \u001b[38;5;241m=\u001b[39m \u001b[43mner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ents:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mner_predict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 22\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Cách khác: dùng encoding.word_ids(batch_index=0) nếu dùng tokenizer fast\u001b[39;00m\n\u001b[1;32m     24\u001b[0m word_ids \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mword_ids(batch_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "test_text = \"Dầu tẩy trang Cocoon hoa hồng cho da dầu mụn\"\n",
    "ents = ner_predict(test_text)\n",
    "for e in ents:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
